{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16ed7aaa-e6c9-42c4-bad6-b5c16176ae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.sse import sse_client\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "from autogen import LLMConfig\n",
    "from autogen.agentchat import AssistantAgent\n",
    "from autogen.mcp import create_toolkit\n",
    "import json\n",
    "import anyio\n",
    "import asyncio\n",
    "\n",
    "# Only needed for Jupyter notebooks\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from autogen.agentchat.group import (\n",
    "    AgentNameTarget,\n",
    "    AgentTarget,\n",
    "    AskUserTarget,\n",
    "    ContextExpression,\n",
    "    ContextStr,\n",
    "    ContextStrLLMCondition,\n",
    "    ContextVariables,\n",
    "    ExpressionAvailableCondition,\n",
    "    ExpressionContextCondition,\n",
    "    GroupChatConfig,\n",
    "    GroupChatTarget,\n",
    "    Handoffs,\n",
    "    NestedChatTarget,\n",
    "    OnCondition,\n",
    "    OnContextCondition,\n",
    "    ReplyResult,\n",
    "    RevertToUserTarget,\n",
    "    SpeakerSelectionResult,\n",
    "    StayTarget,\n",
    "    StringAvailableCondition,\n",
    "    StringContextCondition,\n",
    "    StringLLMCondition,\n",
    "    TerminateTarget,\n",
    ")\n",
    "\n",
    "from autogen.agentchat.group.patterns import (\n",
    "    DefaultPattern,\n",
    "    ManualPattern,\n",
    "    AutoPattern,\n",
    "    RandomPattern,\n",
    "    RoundRobinPattern,\n",
    ")\n",
    "\n",
    "\n",
    "from autogen import ConversableAgent, UpdateSystemMessage\n",
    "from autogen.agents.experimental import DocAgent\n",
    "import os\n",
    "import copy\n",
    "from typing import Any, Dict, List\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from autogen.agentchat import initiate_group_chat, a_initiate_group_chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "56fa18f6-1438-482a-83d6-ef2a04254e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the arxiv MCP server\n",
    "mcp_server_path = Path(\"mcp_news.py\")\n",
    "os.environ[\"OPENAI_API_KEY\"]= \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8d5ccd",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 93\u001b[39m\n\u001b[32m     90\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m create_toolkit_and_run(session)\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     \u001b[43masyncio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/envs/mcp_env/lib/python3.12/site-packages/nest_asyncio.py:30\u001b[39m, in \u001b[36m_patch_asyncio.<locals>.run\u001b[39m\u001b[34m(main, debug)\u001b[39m\n\u001b[32m     28\u001b[39m task = asyncio.ensure_future(main)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m task.done():\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/envs/mcp_env/lib/python3.12/site-packages/nest_asyncio.py:92\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     90\u001b[39m     f._log_destroy_pending = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping:\n\u001b[32m     94\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/envs/mcp_env/lib/python3.12/site-packages/nest_asyncio.py:115\u001b[39m, in \u001b[36m_patch_loop.<locals>._run_once\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    108\u001b[39m     heappop(scheduled)\n\u001b[32m    110\u001b[39m timeout = (\n\u001b[32m    111\u001b[39m     \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ready \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._stopping\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mmin\u001b[39m(\u001b[38;5;28mmax\u001b[39m(\n\u001b[32m    113\u001b[39m         scheduled[\u001b[32m0\u001b[39m]._when - \u001b[38;5;28mself\u001b[39m.time(), \u001b[32m0\u001b[39m), \u001b[32m86400\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m scheduled\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m event_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[38;5;28mself\u001b[39m._process_events(event_list)\n\u001b[32m    118\u001b[39m end_time = \u001b[38;5;28mself\u001b[39m.time() + \u001b[38;5;28mself\u001b[39m._clock_resolution\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Caskroom/miniconda/base/lib/python3.12/selectors.py:566\u001b[39m, in \u001b[36mKqueueSelector.select\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    564\u001b[39m ready = []\n\u001b[32m    565\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m566\u001b[39m     kev_list = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontrol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_ev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    567\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[32m    568\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "twitter_agent = ConversableAgent(\n",
    "    name=\"twitter_agent\",\n",
    "    system_message=\"\"\"You are a Twitter bot agent.\n",
    "You receive news queries from Twitter. Extract the main topic from the tweet and use the MCP news tool to fetch the relevant news article details.\"\"\",\n",
    "    llm_config=LLMConfig(model=\"gpt-4o-mini\", api_type=\"openai\", tool_choice=\"required\")\n",
    ")\n",
    "\n",
    "# News Verification Agent:\n",
    "# Analyzes the provided news article details and determines if the news is real or fake.\n",
    "news_verification_agent = ConversableAgent(\n",
    "    name=\"news_verification_agent\",\n",
    "    system_message=\"\"\"You are a news verification agent.\n",
    "Analyze the provided news article details and decide whether the news is real or fake.\n",
    "Respond with either \"real\" or \"fake\" along with a brief explanation.\"\"\",\n",
    "    llm_config=LLMConfig(model=\"gpt-4o-mini\", api_type=\"openai\", tool_choice=\"required\")\n",
    ")\n",
    "\n",
    "# ---------------------------\n",
    "# Create and run the agent workflow:\n",
    "# ---------------------------\n",
    "async def create_toolkit_and_run(session: ClientSession) -> None:\n",
    "    # Create the MCP toolkit from the session.\n",
    "    toolkit = await create_toolkit(session=session)\n",
    "\n",
    "    # Register MCP tools for both agents.\n",
    "    toolkit.register_for_llm(twitter_agent)\n",
    "    toolkit.register_for_execution(twitter_agent)\n",
    "    toolkit.register_for_llm(news_verification_agent)\n",
    "    toolkit.register_for_execution(news_verification_agent)\n",
    "\n",
    "    # Setup handoffs:\n",
    "    # After the Twitter agent processes the query, pass control to the verification agent.\n",
    "    twitter_agent.handoffs.set_after_work(AgentTarget(news_verification_agent))\n",
    "    news_verification_agent.handoffs.set_after_work(TerminateTarget())\n",
    "\n",
    "    # Simulate receiving a news query from Twitter.\n",
    "    news_query = \"Latest breakthrough in renewable energy\"\n",
    "\n",
    "    # Use the MCP news tool (implemented in mcp_news.py) to fetch a news article.\n",
    "    # Expecting a list with one result.\n",
    "    news_data = await session.call_tool(\"fetch_news\", query=news_query, max_results=5)\n",
    "\n",
    "    # Build a context to pass both the Twitter input and the fetched news article.\n",
    "    workflow_context = ContextVariables(data={\n",
    "        \"twitter_input\": news_query,\n",
    "        \"news_content\": news_content\n",
    "    })\n",
    "\n",
    "    # Create a task instructing the agents to analyze the news details.\n",
    "    task = (\n",
    "        f\"A tweet was received with this news query:\\n'{news_query}'\\n\\n\"\n",
    "        f\"News article details fetched:\\n{news_content}\\n\\n\"\n",
    "        \"Analyze the articles and determine whether the news is real or fake. \"\n",
    "        \"Provide your verdict with a brief explanation.\"\n",
    "    )\n",
    "\n",
    "    # Reset agents before starting the group chat.\n",
    "    for agent in [twitter_agent, news_verification_agent]:\n",
    "        agent.reset()\n",
    "    print(\"Agents reset. Starting group chat analytics...\")\n",
    "\n",
    "    # Create a communication pattern to coordinate the agents.\n",
    "    agent_pattern = DefaultPattern(\n",
    "        agents=[twitter_agent, news_verification_agent],\n",
    "        initial_agent=twitter_agent,\n",
    "        context_variables=workflow_context\n",
    "    )\n",
    "\n",
    "    # Launch the group chat between agents.\n",
    "    await a_initiate_group_chat(\n",
    "        pattern=agent_pattern,\n",
    "        messages=task,\n",
    "        max_rounds=20\n",
    "    )\n",
    "\n",
    "# ---------------------------\n",
    "# Setup and run the MCP server connection:\n",
    "# ---------------------------\n",
    "# Assume the news tool server is implemented in mcp_news.py.\n",
    "server_params = StdioServerParameters(\n",
    "    command=\"python\",\n",
    "    args=[str(mcp_server_path), \"stdio\"]\n",
    ")\n",
    "\n",
    "async def main():\n",
    "    async with stdio_client(server_params) as (read, write), ClientSession(read, write) as session:\n",
    "        await session.initialize()\n",
    "        await create_toolkit_and_run(session)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
